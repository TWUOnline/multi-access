---
title: 'Assessment Processes'
---

> “A grade can be regarded only as an ***inadequate*** report of an ***inaccurate*** judgment by a ***biased*** and ***variable*** judge of the extent to which a student has attained an ***undefined*** level of mastery of an ***unknown*** proportion of an ***indefinite*** material.” Paul Dressel (1957)


The processes of assessment has to do with the design of assessment in a course, the use and scoring of assessment data, and the interpretation and communication of results (DeLuca, 2016).

  - Design  
    - Focuses on the development of reliable assessments and items that measure student learning in relation to learning objectives.  
  - Use/scoring  
    - Focuses of the adjustment and use of scoring protocols and grading schemes to respond to assessment scenarios.  
  - Communication  
    - focuses on the interpretation of assessment results and feedback through communication to students and parents.  

## Designing Assessments

A balanced assessment plan in any given course or program begins in the planning and design phase of a course development project. Recognizing that faculty workload is often prohibitive of a full-scale design process, we provide recommendations here for taking iterative steps towards an assessment strategy that reliably allows for valid inferences to be drawn about learner achievement in relation to learning outcomes.

A primary concern for instructional designers is the constructive alignment (Biggs & Tang, 2011) between learning outcomes and how learner ability is inferred in relation to those learning outcomes. Keep in mind that assessment of learning is never a direct measurement of ability. Constructively aligned course materials exhibit congruence between the cognitive skills required by the intended learning outcomes and the cognitive skills that must be demonstrated during the assessment process. For example, if the verbal phrase in your outcome is something along the lines of "critically analyze", then the assessment task must require critical analysis. If there is a misalignment, then the reliability of the assessment will be diminished and your level of confidence in your interpretation of the data should be lowered as well. According to Biggs and Tang (2011), misalignment often leads to learners opting for surface approaches to their learning where they take shortcuts and rely on low-level cognitive skills (memorization) when high-level cognitive skills (critical analysis) are required, rather than taking a deep approach by using high-level cognitive skills where they are required.

An example of misalignment can be seen in the situation where a learning outcome requires critical analysis, but the assessment task only requires learners to recognize a correct answer on a selected-response test. While it is not impossible to assess a learner's ability to critically analyze a construct using a selected-response test, such tests are difficult to design.

Another priority in the design of an assessment plan is that there are adequate opportunities for learners to demonstrate their ability throughout the course. 


[Alternative Assessment](https://www.ryerson.ca/content/dam/learning-teaching/teaching-resources/assessment/alternative-assessments.pdf)

## Use and Scoring

The use and scoring of assessment data is not entirely as straightforward as it might seem. As we have identified, there are numerous factors involved in determining what a learner actually knows and can do in relation to a particular learning outcome. As such, we should take care with how we interpret assessment data and also how we report it to learners and the university.

A good starting point is to think about the scales that we use to score and communicate achievement. Likely the most common is a 0-100, or percentage scale, where a particular submission is assigned a numerical grade between 0 and 100 points. An alternative to this is the submission is assigned a number on a different scale (perhaps related to the number of items on an assessment) which is then converted to a percentage for reporting purposes. This strategy is somewhat problematic in that, based on what we know from classical test theory, every assessment task and situation contains any number of sources of error ($E$). What this means is that the tools we use to provide assessment data are imprecise, yet the scale we use to communicate inferences based on that data *seem* precise. In fact, that precision is illusory. A simple thought experiment can show the difficulty.

On a 0-100 scale, there are 101 possible grades, 50 of which are considered failing grades (or 80 in graduate-level courses). Given that, the likelihood of getting a learner's grade wrong is quite high. In fact, you will almost certainly get it wrong. This is true even if your test is entirely selected-response and there is only one possible correct response for each item because there is *always* measurement error. Keep in mind that if you use a 0-100 scale and also report one decimal place, then you have 1010 possible gradations of quality.

The [TWU University Standard Grading System](https://www.twu.ca/about/policies-guidelines/university-standard-grading-system), last revised in 1992, according to the webpage, is as follows:

! The grading scale for your program may be different than this!

| Letter Grade | Percentage | Grade Point |
|:---:|:---:|:---:|
| A+ | 90-100 | 4.3 |
| A | 85-89 | 4.0 |
| A- | 80-84 | 3.7 |
| B+ | 77-79 | 3.3 |
| B | 73-76 | 3.0 |
| B- | 70-72 | 2.7 |
| C+ | 67-69 | 2.3 |
| C | 63-66 | 2.0 |
| C- | 60-62 | 1.7 |
| D+ | 57-59 | 1.3 |
| D | 53-56 | 1.0 |
| D- | 50-52 | 0.7 |
| F | <50 | 0 |

I wasn't around (except as a first-year TWU student...) in 1992 to know the details of the conversation about the grading scale, but it does lead to some interesting observations.

- There is no difference between any score from 0 and 49 in terms of GPA or passing a course. A score of 49 is identical to a score of 0.  
- The distance from a  high B to a low A (76-85) is 9 points, but the distance from a high C to a low B (66-73) is 7 points, yet the GPAs between each of those is exactly 1.  
- The A range of scores (21 points) is more than twice the size of every other range (9 points), yet a score of A should be rarer than other grades.  
- An A+ (11 point range) is almost 4 times larger than every other '+' grade (3 points)  
- The distance from an F to a B- is 21 points

At the bottom of it all, assigning these particular percentage ranges to these particular letter grades is arbitrary. The University recognizes this in allowing departments or individual instructors to deviate from this scale. However, only the percentage equivalents are subject to change, meaning that the letters assigned still retain the same weight on the GPA scale.

For example, here is another published scale from a TWU undergraduate course, the Modified Course...

| Letter Grade | Percentage | Grade Point |
|:---:|:---:|:---:|
| A+ | 98-100 | 4.3 |
| A | 94-97 | 4.0 |
| A- | 90-93 | 3.7 |
| B+ | 87-89 | 3.3 |
| B | 83-86 | 3.0 |
| B- | 80-82 | 2.7 |
| C+ | 75-79 | 2.3 |
| C | 70-74| 2.0 |
| C- | 65-69 | 1.7 |
| D+ | 60-64 | 1.3 |
| D | 55-59 | 1.0 |
| D- | 50-54 | 0.7 |
| F | <50 | 0 |

Some things to notice here...

- The A (11 points) and B (10 points) ranges are very similar in size.
- The C and D ranges are both 15 points
- The distance from an F to a B- is 31 points

This second grading scale makes it more difficult for a slower learner to climb out of the hole of early low grades. Furthermore, when grades are calculated on a percentage scale, and then reported as a letter grade, any ability to compare a grade from the University Standard Scale to a grade from a learner in the second course is completely illusory. This makes it impossible to speak meaningfully about what an A grade means at TWU. This has very real implications for learners. A learner in the Modified Course who is assigned a 79, a C+ or 2.3 GPA, while a learner in a Standard course who is assigned a 79, is a solid B+ or 3.3 GPA. If each of these learners were to apply to a graduate program at TWU or anywhere else, the learner in a Standard course is far more likely to be admitted than a learner who, theoretically, performed identically well. However, this fact is completely hidden due to the fact that grades are reported as letters. Learners in the Modified course are at a significant disadvantage in applying to grad school. 

Therein lies a significant problem with letter grade scales. They hide too much context.

The best way around these contradictions is to reduce the number of gradations of quality by abandoning the percentage or 0-100 points system. The letter grade scale does just that, with only 13 gradations of quality. With fewer grade categories, instructors are far more likely to accurately place a learner in the correct category, making this a *more* reliable scale than 0-100.

Fortunately, TWU has published, on the same page as the letter grade/percentage equivalents, a series of [proficiency indicators to provide context to letter grades.](https://www.twu.ca/about/policies-guidelines/university-standard-grading-system)

| Letter Grade | Quality Characteristics | 
|:---: | --- |
| A | **Outstanding, excellent work**; exceptional performance with strong evidence of original thinking, good organization, meticulous concern for documented evidence, and obvious capacity to analyze, synthesize, evaluate, discern, justify, and elaborate; frequent evidence of both verbal eloquence and perceptive insight in written expression; excellent problem-solving ability in scientific or mathematical contexts with virtually no computational errors; demonstrated masterful grasp of subject matter and its implications. Gives evidence of an extensive and detailed knowledge base. (Note: The A+ grade is reserved for very rare students of exceptional intellectual prowess and accomplishment, especially in lower level courses.) |
| B | **Good, competent work**; laudable performance with evidence of some original thinking, careful organization; satisfactory critical and analytical capacity; reasonably error-free expository written expression, with clear, focused thesis and well-supported, documented, relevant arguments; good problem-solving ability, with few computational or conceptual errors in scientific subjects; reasonably good grasp of subject matter but an occasional lack of depth of discernment; evidence of reasonable familiarity with course subject matter, both concepts and key issues. Exhibits a serious, responsible engagement with the course content. |
| C | **Adequate, reasonably satisfactory work**; fair performance but infrequent evidence of original thinking or the capacity to analyze, synthesize, or evaluate course material; undue reliance on rote memory; difficulty in applying knowledge in unfamiliar contexts; limited problem-solving ability in scientific subjects; fairly clear but quite uninspiring written expression with occasional problems in mechanics or syntax; weak in provision of documented, illustrative, or descriptive evidence; satisfactory grasp of basic elements of the course but frequent lapses in detailed understanding. Satisfies the minimum requirements of the course. |
| D | **Minimally acceptable work**; relatively weak performance with little evidence of original thinking or ability to analyze or synthesize course material; nominal or weak problem-solving ability in scientific subjects; written expression frequently exhibits difficulty in articulating a central thesis or sustaining a coherent argument; ideas are trite or juvenile, without discernible development. Shows inadequate grasp of some basic elements of the course. |
| F |  	**Inadequate work**; poor performance that indicates a lack of understanding or misunderstanding of essential subject matter; seems easily distracted by the irrelevant; written expression is poorly organized, often incoherent, and rife with mechanical and diction errors. Shows little evidence of even basic competency in the course content or skills. |

This categorization, with only five levels of proficiency, significantly increases the likelihood that you will be able to accurately characterize each learner's performance in your course. Notably, this is also the type of grade reporting universities will be getting from secondary schools in the coming years, so we would be wise to start thinking about how to incorporate these types of reports into our assessment vocabulary.

## The Problem(s) with Averages

A common method of coming to a conclusion about a final grade is to calculate an average of scores from throughout the semester and report that as a summative determination. There are a couple of problems with that approach. The first is that assessment data is ***ordinal*** data.

<a class="embedly-card" href="https://www.chi2innovations.com/wp-content/uploads/2020/11/Ordinal-Data.png">Card</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

In the discussion above, we have established that assessment data is not measured, data points are not equidistant from each other, and there is no meaningful zero, but it is 'ordered'. We know that A is better than B, but we don't know how much better and we know that the relationship between A/B is not the same as the relationship between B/C, or C/D. This problem doesn't disappear if we rely on the 0-100 scale because we know that we are not actually 'measuring' anything. This means that you are limited in what you can do with assessment data, as below:

<a class="embedly-card" href="https://www.chi2innovations.com/wp-content/uploads/2020/11/Ordinal-Data-Mathematical-Features-Wide-768x587.png.webp">Card</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

Since you can't perform mathematical calculations with ordinal data, you can't calculate an average unless you make certain assumptions.

Another problem with averages is that averaging learner achievement over a semester penalizes learners who learn the most. Consider the table below which shows assessment data on cumulative assignments as percentages for four learners which is then averaged for a final grade:

| Learner | Item 1 | Item 2 | Item 3 | Item 4 | Final |
|:---:|:---:|:---:|:---:|:---:|:---:|
| A | 75% | 75% | 75% | 75% | 75% | 
| B | 25% | 75% | 100% | 100% | 75% |
| C | 100% | 100% | 75% | 25% | 75% |
| D | 0% | 100% | 100% | 100% | 75% |

It would seem that Learner A is accurately categorized at 75% due to the consistency with which they scored 75% on assessment tasks. Learner B, however started the semester with low scores, then finished very strongly. They made very rapid and stable improvement during the semester, yet couldn't score higher than 75% due to one poor grade. Learner C's performance was the opposite of that. They started very strong and their performance decreased drastically over the semester, seemingly indicating that they could not understand the integrated whole of the course. Finally, Learner D completely missed the first assessment, wasn't allowed to make it up, then scored perfectly for the rest of the semester, leaving them with 75%. This is a goo demonstration of how the mean of a data set is strongly influenced by outliers. If the instructor were to choose to use a different measure of central tendency (which ***can*** be determined with ordinal data),  or the learner's most recent grade, the calculations could be as follows:

| Learner | Mean | Median | Mode | Most Recent|
|:---:|:---:|:---:|:---:|:---:|
| A | 75% | 75% | 75% | 75% |
| B | 75% | 100% | 100% | 100% |
| C | 75% | 100% | 100% | 25% | 
| D | 75% | 100% | 100% | 100% |

In the end, we are left with no good way to determine what a learner knows or can do...unless, of course, we significantly reduce the number of proficiency categories and stop trying to do math with ordinal data.



<!--
- normal curves 
- averages
- zeros
- scales-->




# References

Biggs, J., & Tang, C. (2011). *Teaching for quality learning at university: What the student does (4th ed.)*. Society for Research into Higher Education & Open University Press.

DeLuca, C., Valiquette, A., Coombs, A., LaPointe-McEwan, D., & Luhanga, U. (2016). [Teachers’ approaches to classroom assessment: A large-scale survey](https://doi.org/10/gh5k6p). *Assessment in Education: Principles, Policy & Practice, 25*, 355–375. 
