---
title: 'Assessment Processes'
---

> “A grade can be regarded only as an *inadequate* report of an *inaccurate* judgment by a *biased* and *variable* judge of the extent to which a student has attained an *undefined* level of mastery of an *unknown* proportion of an *indefinite* material.” Paul Dressel (1957)


The processes of assessment has to do with the design of assessment in a course, the use and scoring of assessment data, and the interpretation and communication of results (DeLuca, 2016).

  - Design  
    - Focuses on the development of reliable assessments and items that measure student learning in relation to learning objectives.  
  - Use/scoring  
    - Focuses of the adjustment and use of scoring protocols and grading schemes to respond to assessment scenarios.  
  - Communication  
    - focuses on the interpretation of assessment results and feedback through communication to students and parents.  
## Designing Assessments

A balanced assessment plan in any given course or program begins in the planning and design phase of a course development project. Recognizing that faculty workload is often prohibitive of a full-scale design process, we provide recommendations here for taking iterative steps towards an assessment strategy that reliably allows for valid inferences to be drawn about learner achievement in relation to learning outcomes.

A primary concern for instructional designers is the constructive alignment (Biggs & Tang, 2011) between learning outcomes and how learner ability is inferred in relation to those learning outcomes. Keep in mind that assessment of learning is never a direct measurement of ability. Constructively aligned course materials exhibit congruence between the cognitive skills required by the intended learning outcomes and the cognitive skills that must be demonstrated during the assessment process. For example, if the verbal phrase in your outcome is something along the lines of "critically analyze", then the assessment task must require critical analysis. If there is a misalignment, then the reliability of the assessment will be diminished and your level of confidence in your interpretation of the data should be lowered as well. According to Biggs and Tang (2011), misalignment often leads to learners opting for surface approaches to their learning where they take shortcuts and rely on low-level cognitive skills (memorization) when high-level cognitive skills (critical analysis) are required, rather than taking a deep approach by using high-level cognitive skills where they are required.

An example of misalignment can be seen in the situation where a learning outcome requires critical analysis, but the assessment task only requires learners to recognize a correct answer on a selected-response test. While it is not impossible to assess a learner's ability to critically analyze a construct using a selected-response test, such tests are difficult to design.

Another priority in the design of an assessment plan is that there are adequate opportunities for learners to demonstrate their ability throughout the course. 


[Alternative Assessment](https://www.ryerson.ca/content/dam/learning-teaching/teaching-resources/assessment/alternative-assessments.pdf)

## Use and Scoring

The use and scoring of assessment data is not entirely as straightforward as it might seem. As we have identified, there are numerous factors involved in determining what a learner actually knows and can do in relation to a particular learning outcome. As such, we should take care with how we interpret assessment data and also how we report it to learners and the university.

A good starting point is to think about the scales that we use to score and communicate achievement. Likely the most common is a 0-100, or percentage scale, where a particular submission is assigned a numerical grade between 0 and 100 points. An alternative to this is the submission is assigned a number on a different scale (perhaps related to the number of items on an assessment) which is then converted to a percentage for reporting purposes. This strategy is somewhat problematic in that, based on what we know from classical test theory, every assessment task and situation contains any number of sources of error ($E$). What this means is that the tools we use to provide assessment data are imprecise, yet the scale we use to communicate inferences based on that data *seem* precise. In fact, that precision is illusory. A simple thought experiment can show the difficulty.

On a 0-100 scale, there are 101 possible grades, 50 of which are considered failing grades (or 80 in graduate-level courses). Given that, the likelihood of getting a learner's grade wrong is quite high. In fact, you will almost certainly get it wrong. This is true even if your test is entirely selected-response and there is only one possible correct response for each item because there is *always* measurement error. Keep in mind that if you use a 0-100 scale and also report one decimal place, then you have 1010 possible gradations of quality.

The [TWU University Standard Grading System](https://www.twu.ca/about/policies-guidelines/university-standard-grading-system), last revised in 1992, according to the webpage, is as follows:

! The grading scale for your program may be different than this!

| Letter Grade | Percentage | Grade Point |
|:---:|:---:|:---:|
| A+ | 90-100 | 4.3 |
| A | 85-89 | 4.0 |
| A- | 80-84 | 3.7 |
| B+ | 77-79 | 3.3 |
| B | 73-76 | 3.0 |
| B- | 70-72 | 2.7 |
| C+ | 67-69 | 2.3 |
| C | 63-66 | 2.0 |
| C- | 60-62 | 1.7 |
| D+ | 57-59 | 1.3 |
| D | 53-56 | 1.0 |
| D- | 50-52 | 0.7 |
| F | <50 | 0 |

I wasn't around (except as a first-year TWU student...) in 1992 to know the details of the conversation about the grading scale, but it does lead to some interesting observations.

- There is no difference between any score from 0 and 49 in terms of GPA or passing a course. A score of 49 is identical to a score of 0.  
- The distance from a  high B to a low A (76-85) is 9 points, but the distance from a high C to a low B (66-73) is 7 points, yet the GPAs between each of those is exactly 1.  
- The A range of scores (21 points) is more than twice the size of every other range (9 points), yet a score of A should be rarer than other grades.  
- An A+ (11 point range) is almost 4 times larger than every other '+' grade (3 points)  
- The distance from an F to a B- is 21 points

At the bottom of it all, assigning these particular percentage ranges to these particular letter grades is arbitrary. The University recognizes this in allowing departments or individual instructors to deviate from this scale. However, only the percentage equivalents are subject to change, meaning that the letters assigned still retain the same weight on the GPA scale.

For example, here is another published scale from a TWU undergraduate course, the Modified Course...

| Letter Grade | Percentage | Grade Point |
|:---:|:---:|:---:|
| A+ | 98-100 | 4.3 |
| A | 94-97 | 4.0 |
| A- | 90-93 | 3.7 |
| B+ | 87-89 | 3.3 |
| B | 83-86 | 3.0 |
| B- | 80-82 | 2.7 |
| C+ | 75-79 | 2.3 |
| C | 70-74| 2.0 |
| C- | 65-69 | 1.7 |
| D+ | 60-64 | 1.3 |
| D | 55-59 | 1.0 |
| D- | 50-54 | 0.7 |
| F | <50 | 0 |

Some things to notice here...

- The A (11 points) and B (10 points) ranges are very similar in size.
- The C and D ranges are both 15 points
- The distance from an F to a B- is 31 points

This second grading scale makes it more difficult for a slower learner to climb out of the hole of early low grades. Furthermore, when grades are calculated on a percentage scale, and then reported as a letter grade, any ability to compare a grade from the University Standard Scale to a grade from a learner in the second course is completely illusory. This makes it impossible to speak meaningfully about what an A grade means at TWU. This has very real implications for learners. A learner in the Modified Course who is assigned a 79, or C+ or 2.3 GPA, while a learner in a Standard course who is assigned a 79, is a solid B+ or 3.3 GPA. If each of these learners were to apply to a graduate program at TWU, the learner in a Standard course is far more likely to be admitted than a learner who, theoretically, performed identically well.

The best way around these contradictions is to reduce the number of gradations of quality by abandoning the percentage or 0-100 points system. The letter grade scale does just that, with only 13 gradations of quality. With fewer grade categories, instructors are far more likely to accurately place a learner in the correct category, making this a *more* reliable scale than 0-100.
<!--
- normal curves 
- averages
- zeros
- scales-->
### Grading Rubrics



<iframe title="vimeo-player" src="https://player.vimeo.com/video/702950788?h=801b22cb5e" width="640" height="360" frameborder="0" allowfullscreen></iframe>

<br>


<iframe title="vimeo-player" src="https://player.vimeo.com/video/702949944?h=d46428e2c1" width="640" height="360" frameborder="0" allowfullscreen></iframe>
<br>


<iframe title="vimeo-player" src="https://player.vimeo.com/video/702949118?h=c242a28cb5" width="640" height="360" frameborder="0" allowfullscreen></iframe>



# References

Biggs, J., & Tang, C. (2011). *Teaching for quality learning at university: What the student does (4th ed.)*. Society for Research into Higher Education & Open University Press.

DeLuca, C., Valiquette, A., Coombs, A., LaPointe-McEwan, D., & Luhanga, U. (2016). [Teachers’ approaches to classroom assessment: A large-scale survey](https://doi.org/10/gh5k6p). *Assessment in Education: Principles, Policy & Practice, 25*, 355–375. 
